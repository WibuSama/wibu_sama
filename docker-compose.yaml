# -*- coding: utf-8 -*-
"""
FastAPI + MetaGPT backend with:
- /init: Start clarification session
- /ws/{session_id}: WebSocket to clarify with virtual PM (JSON questions)
- /confirm/{session_id}: Confirm and trigger MetaGPT team
- /result/{session_id}: Retrieve result
- Serves static UI from /static/index.html
"""
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
import asyncio
import json
import os
import re
from datetime import datetime

# ==========================
# REFLECTIVE LLM AGENT CORE
# ==========================

# ƒê·ªãnh nghƒ©a c√°c tr·∫°ng th√°i c·ªßa orchestrator
class OrchestratorState:
    INITIALIZING = "initializing"
    ANALYZING = "analyzing"
    ASKING_QUESTION = "asking_question"
    EVALUATING_ANSWER = "evaluating_answer"
    ENRICHING_CONTEXT = "enriching_context"
    PLANNING_NEXT_ACTION = "planning_next_action"
    SUMMARIZING = "summarizing"
    FINALIZING = "finalizing"
    ERROR = "error"

class LLMAction:
    ASK_FOLLOW_UP = "ask_follow_up"
    CALL_TOOL = "call_tool"
    CONCLUDE = "conclude"
    SUMMARIZE = "summarize"
    SEARCH_MORE = "search_more"
    REFINE_UNDERSTANDING = "refine_understanding"
    REQUEST_SPECIFIC_INFO = "request_specific_info"

# C·∫•u tr√∫c cho m·ªôt cycle c·ªßa orchestrator
class OrchestrationCycle:
    def __init__(self, state: str, context: Dict[str, Any]):
        self.state = state
        self.context = context
        self.start_time = datetime.utcnow()
        self.end_time = None
        self.reasoning = []
        self.tools_called = []
        self.llm_actions = []
        self.summary = None
        self.next_state = None
        
    def add_reasoning(self, reasoning: str):
        """Th√™m m·ªôt b∆∞·ªõc l√Ω lu·∫≠n v√†o cycle hi·ªán t·∫°i"""
        timestamp = datetime.utcnow().isoformat()
        self.reasoning.append({"timestamp": timestamp, "reasoning": reasoning})
        
    def add_tool_call(self, tool_name: str, params: Dict[str, Any], result: Any):
        """L∆∞u l·∫°i th√¥ng tin v·ªÅ c√¥ng c·ª• ƒë√£ g·ªçi"""
        timestamp = datetime.utcnow().isoformat()
        self.tools_called.append({
            "timestamp": timestamp,
            "tool": tool_name,
            "params": params,
            "result_summary": str(result)[:100] + "..." if isinstance(result, str) and len(str(result)) > 100 else result
        })
    
    def add_llm_action(self, action_type: str, details: Dict[str, Any]):
        """L∆∞u l·∫°i h√†nh ƒë·ªông c·ªßa LLM"""
        timestamp = datetime.utcnow().isoformat()
        self.llm_actions.append({
            "timestamp": timestamp,
            "action_type": action_type,
            "details": details
        })
    
    def complete(self, next_state: str, summary: str):
        """Ho√†n th√†nh cycle hi·ªán t·∫°i"""
        self.end_time = datetime.utcnow()
        self.next_state = next_state
        self.summary = summary

class LLMOrchestrator:
    """
    LLM-driven Orchestration Engine - t·ª± ƒë·ªông h√≥a qu√° tr√¨nh ph√¢n t√≠ch, ƒë√°nh gi√° v√†
    ƒë∆∞a ra quy·∫øt ƒë·ªãnh v·ªÅ c√°c b∆∞·ªõc ti·∫øp theo trong qu√° tr√¨nh l√†m r√µ y√™u c·∫ßu.
    """
    
    def __init__(self, llm_instance=None):
        """
        Kh·ªüi t·∫°o orchestrator
        
        Args:
            llm_instance: Instance c·ªßa LLM ƒë·ªÉ s·ª≠ d·ª•ng, n·∫øu kh√¥ng c√≥ s·∫Ω t·∫°o m·ªõi
        """
        from metagpt.llm import LLM
        self.llm = llm_instance or LLM()
        self.cycles = []
        self.current_cycle = None
        self.session_data = {
            "requirement": None,
            "chat_history": [],
            "technical_details": {},
            "context_enrichments": [],
            "final_summary": None,
            "ready_to_conclude": False
        }
    
    def _start_new_cycle(self, state: str):
        """B·∫Øt ƒë·∫ßu m·ªôt cycle m·ªõi v·ªõi tr·∫°ng th√°i ƒë√£ cho"""
        # L∆∞u cycle hi·ªán t·∫°i v√†o history n·∫øu c√≥
        if self.current_cycle:
            self.cycles.append(self.current_cycle)
            
        # T·∫°o cycle m·ªõi
        self.current_cycle = OrchestrationCycle(
            state=state,
            context={
                "timestamp": datetime.utcnow().isoformat(),
                "requirement": self.session_data["requirement"],
                "chat_history_length": len(self.session_data["chat_history"])
            }
        )
        return self.current_cycle
    
    async def initialize(self, requirement: str, team_data: Optional[List[Dict]] = None):
        """
        Kh·ªüi t·∫°o session v·ªõi y√™u c·∫ßu ban ƒë·∫ßu
        
        Args:
            requirement: Y√™u c·∫ßu ban ƒë·∫ßu t·ª´ ng∆∞·ªùi d√πng
            team_data: Th√¥ng tin v·ªÅ team n·∫øu c√≥
        """
        self._start_new_cycle(OrchestratorState.INITIALIZING)
        self.session_data["requirement"] = requirement
        self.session_data["team_data"] = team_data or []
        
        # Ph√¢n t√≠ch y√™u c·∫ßu ban ƒë·∫ßu
        self.current_cycle.add_reasoning("B·∫Øt ƒë·∫ßu phi√™n l√†m r√µ y√™u c·∫ßu")
        
        # T·ª± ƒë·ªông l√†m gi√†u ng·ªØ c·∫£nh v·ªõi MCP n·∫øu c√≥ th·ªÉ
        if USE_MCP_ENRICHMENT:
            self.current_cycle.add_reasoning("L√†m gi√†u ng·ªØ c·∫£nh ban ƒë·∫ßu v·ªõi MCP tools")
            self.current_cycle.state = OrchestratorState.ENRICHING_CONTEXT
            
            try:
                # Ph√¢n t√≠ch y√™u c·∫ßu ban ƒë·∫ßu ƒë·ªÉ l√†m gi√†u ng·ªØ c·∫£nh
                enriched_data = await enrich_context_with_mcp(requirement, [])
                self.session_data["context_enrichments"].append({
                    "timestamp": datetime.utcnow().isoformat(),
                    "enrichment_data": enriched_data
                })
                
                # L∆∞u th√¥ng tin v·ªÅ c√¥ng c·ª• ƒë√£ g·ªçi
                self.current_cycle.add_tool_call(
                    "enrich_context_with_mcp", 
                    {"text": requirement}, 
                    "L√†m gi√†u ng·ªØ c·∫£nh ban ƒë·∫ßu"
                )
                
                # Th√™m reasoning
                for reasoning in enriched_data.get("reasoning", []):
                    self.current_cycle.add_reasoning(reasoning)
            except Exception as e:
                self.current_cycle.add_reasoning(f"L·ªói khi l√†m gi√†u ng·ªØ c·∫£nh ban ƒë·∫ßu: {str(e)}")
        
        self.current_cycle.complete(
            OrchestratorState.ASKING_QUESTION,
            "Kh·ªüi t·∫°o session th√†nh c√¥ng"
        )
    
    async def generate_first_question(self) -> Dict[str, Any]:
        """
        Sinh c√¢u h·ªèi ƒë·∫ßu ti√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu l√†m r√µ y√™u c·∫ßu
        
        Returns:
            Dict ch·ª©a c√¢u h·ªèi ch√≠nh v√† c√°c c√¢u h·ªèi l√†m r√µ
        """
        self._start_new_cycle(OrchestratorState.ASKING_QUESTION)
        self.current_cycle.add_reasoning("Sinh c√¢u h·ªèi ƒë·∫ßu ti√™n")
        
        # X√¢y d·ª±ng prompt
        prompt = await self._build_first_question_prompt()
        
        # G·ªçi LLM ƒë·ªÉ sinh c√¢u h·ªèi ƒë·∫ßu ti√™n
        llm_result = await self.llm.aask(prompt)
        self.current_cycle.add_llm_action(LLMAction.ASK_FOLLOW_UP, {"prompt": "First question", "result": llm_result[:100] + "..."})
        
        # Parse k·∫øt qu·∫£ t·ª´ LLM (kh√¥ng fallback)
        clean_response = llm_result.strip()
        if clean_response.startswith("\n") and clean_response.endswith("\n"):
            clean_response = clean_response[3:-3].strip()
            if clean_response.startswith("json"):
                clean_response = clean_response[4:].strip()
        
        question_data = json.loads(clean_response)
        self.session_data["last_question"] = question_data["main_question"]
        
        self.current_cycle.complete(
            OrchestratorState.ASKING_QUESTION,
            "ƒê√£ sinh c√¢u h·ªèi ƒë·∫ßu ti√™n th√†nh c√¥ng"
        )
        
        return question_data
            
    async def _build_first_question_prompt(self) -> str:
        """X√¢y d·ª±ng prompt cho c√¢u h·ªèi ƒë·∫ßu ti√™n"""
        # L·∫•y enrichment data g·∫ßn ƒë√¢y nh·∫•t n·∫øu c√≥
        enrichment_text = ""
        if USE_MCP_ENRICHMENT and self.session_data["context_enrichments"]:
            latest_enrichment = self.session_data["context_enrichments"][-1]["enrichment_data"]
            enrichment_text = format_enriched_context(latest_enrichment)
        
        prompt = f"""
B·∫°n l√† K·ªπ s∆∞ Tr∆∞·ªüng. H√£y ƒë·ªçc y√™u c·∫ßu sau v√† sinh ra 1 c√¢u h·ªèi ch√≠nh ƒë·ªÉ l√†m r√µ v·ªÅ c√°c chi ti·∫øt k·ªπ thu·∫≠t, k√®m 2-3 c√¢u h·ªèi ph·ª• n·∫øu c·∫ßn.

Y√äU C·∫¶U: {self.session_data['requirement']}
TH√îNG TIN TEAM: {json.dumps(self.session_data['team_data'], ensure_ascii=False)}

{enrichment_text}

H∆Ø·ªöNG D·∫™N CHI TI·∫æT:
1. T·∫≠p trung v√†o c√°c kh√≠a c·∫°nh TRI·ªÇN KHAI K·ª∏ THU·∫¨T v√† CHI TI·∫æT PH√ÅT TRI·ªÇN c·ª• th·ªÉ
2. H·ªèi v·ªÅ stack c√¥ng ngh·ªá, ki·∫øn tr√∫c, database, API, integration, workflow k·ªπ thu·∫≠t...
3. H·ªèi chi ti·∫øt ƒë·∫øn m·ª©c c√°c class/function quan tr·ªçng n·∫øu c√≥ th·ªÉ
4. TR√ÅNH h·ªèi v·ªÅ c√°c gi·ªõi h·∫°n (limit) ho·∫∑c th·ªùi gian x·ª≠ l√Ω t·ªëi ƒëa
5. H·ªèi v·ªÅ c√°c requirement k·ªπ thu·∫≠t chi ti·∫øt m√† developer c·∫ßn ƒë·ªÉ tri·ªÉn khai

**B·∫ÆT BU·ªòC**: Ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, kh√¥ng s·ª≠ d·ª•ng ti·∫øng Anh, kh√¥ng gi·∫£i th√≠ch th√™m, kh√¥ng markdown/code block, kh√¥ng th√™m b·∫•t k·ª≥ k√Ω t·ª± n√†o ngo√†i JSON.

Tr·∫£ v·ªÅ JSON:
{{
  "main_question": "...",
  "clarifying_questions": ["...", "..."]
}}
"""
        return prompt
    
    async def process_user_answer(self, user_answer: str, last_question: str) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng, l√†m gi√†u ng·ªØ c·∫£nh, v√† quy·∫øt ƒë·ªãnh h√†nh ƒë·ªông ti·∫øp theo
        
        Args:
            user_answer: C√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng
            last_question: C√¢u h·ªèi cu·ªëi c√πng ƒë∆∞·ª£c h·ªèi
            
        Returns:
            Dict ch·ª©a th√¥ng tin v·ªÅ action ti·∫øp theo v√† c√°c c√¢u h·ªèi m·ªõi (n·∫øu c√≥)
        """
        self._start_new_cycle(OrchestratorState.EVALUATING_ANSWER)
        self.current_cycle.add_reasoning(f"X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng: '{user_answer[:50]}...'")
        
        # L∆∞u c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi v√†o chat history
        self.session_data["chat_history"].append({
            "question": last_question,
            "answer": user_answer,
            "timestamp": datetime.utcnow().isoformat()
        })
        
        # 1. L√†m gi√†u ng·ªØ c·∫£nh v·ªõi MCP tools n·∫øu c√≥ th·ªÉ
        if USE_MCP_ENRICHMENT:
            self.current_cycle.state = OrchestratorState.ENRICHING_CONTEXT
            try:
                # L√†m gi√†u ng·ªØ c·∫£nh v·ªõi c√¥ng c·ª• MCP
                self.current_cycle.add_reasoning("L√†m gi√†u ng·ªØ c·∫£nh v·ªõi MCP tools")
                
                enriched_data = await enrich_context_with_mcp(
                    user_answer=user_answer,
                    chat_history=self.session_data["chat_history"]
                )
                
                # L∆∞u k·∫øt qu·∫£ enrichment
                self.session_data["context_enrichments"].append({
                    "timestamp": datetime.utcnow().isoformat(),
                    "enrichment_data": enriched_data
                })
                
                # L∆∞u th√¥ng tin v·ªÅ c√¥ng c·ª• ƒë√£ g·ªçi
                self.current_cycle.add_tool_call(
                    "enrich_context_with_mcp", 
                    {"text": user_answer}, 
                    "L√†m gi√†u ng·ªØ c·∫£nh"
                )
                
                # Th√™m reasoning t·ª´ qu√° tr√¨nh l√†m gi√†u ng·ªØ c·∫£nh
                for reasoning in enriched_data.get("reasoning", []):
                    self.current_cycle.add_reasoning(reasoning)
                
            except Exception as e:
                self.current_cycle.add_reasoning(f"L·ªói khi l√†m gi√†u ng·ªØ c·∫£nh: {str(e)}")
        
        # 2. ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi v√† l·∫≠p k·∫ø ho·∫°ch h√†nh ƒë·ªông ti·∫øp theo
        self.current_cycle.state = OrchestratorState.PLANNING_NEXT_ACTION
        evaluation_result = await self._evaluate_and_plan_next_action(user_answer, last_question)
        
        # 3. Th·ª±c hi·ªán h√†nh ƒë·ªông ti·∫øp theo d·ª±a tr√™n k·∫øt qu·∫£ ƒë√°nh gi√°
        next_action = evaluation_result.get("next_action", LLMAction.ASK_FOLLOW_UP)
        
        if next_action == LLMAction.CONCLUDE:
            # K·∫øt th√∫c qu√° tr√¨nh l√†m r√µ y√™u c·∫ßu
            self.session_data["ready_to_conclude"] = True
            self.session_data["final_summary"] = evaluation_result.get("updated_summary", "")
            
            self.current_cycle.complete(
                OrchestratorState.FINALIZING,
                "Ho√†n th√†nh qu√° tr√¨nh l√†m r√µ y√™u c·∫ßu"
            )
            
            return {
                "action": "conclude",
                "updated_summary": evaluation_result.get("updated_summary", ""),
                "technical_analysis": evaluation_result.get("technical_analysis", {}),
                "is_completed": True
            }
            
        else:
            # Sinh c√¢u h·ªèi ti·∫øp theo d·ª±a tr√™n ƒë√°nh gi√°
            next_question = evaluation_result.get("next_question", "")
            clarifying_questions = evaluation_result.get("clarifying_questions", [])
            
            self.current_cycle.complete(
                OrchestratorState.ASKING_QUESTION,
                f"Chu·∫©n b·ªã c√¢u h·ªèi ti·∫øp theo: '{next_question[:50]}...'"
            )
            
            return {
                "action": "ask_follow_up",
                "main_question": next_question,
                "clarifying_questions": clarifying_questions,
                "updated_summary": evaluation_result.get("updated_summary", ""),
                "is_completed": False
            }
    
    async def _evaluate_and_plan_next_action(self, user_answer: str, last_question: str) -> Dict[str, Any]:
        """
        ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng v√† l·∫≠p k·∫ø ho·∫°ch h√†nh ƒë·ªông ti·∫øp theo
        
        Args:
            user_answer: C√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng
            last_question: C√¢u h·ªèi cu·ªëi c√πng ƒë∆∞·ª£c h·ªèi
            
        Returns:
            Dict v·ªõi th√¥ng tin ƒë√°nh gi√° v√† k·∫ø ho·∫°ch h√†nh ƒë·ªông
        """
        # X√¢y d·ª±ng prompt d·ª±a tr√™n l·ªãch s·ª≠ v√† ng·ªØ c·∫£nh l√†m gi√†u
        prompt = await self._build_evaluation_prompt(user_answer, last_question)
        
        # G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch v√† ƒë∆∞a ra quy·∫øt ƒë·ªãnh
        llm_result = await self.llm.aask(prompt)
        self.current_cycle.add_llm_action(LLMAction.REFINE_UNDERSTANDING, {"prompt": "Evaluate and plan next action", "result": llm_result[:100] + "..."})
        
        # Parse k·∫øt qu·∫£ t·ª´ LLM
        clean_response = llm_result.strip()
        if clean_response.startswith("\n") and clean_response.endswith("\n"):
            clean_response = clean_response[3:-3].strip()
            if clean_response.startswith("json"):
                clean_response = clean_response[4:].strip()
        
        evaluation_result = json.loads(clean_response)
        
        # C·∫≠p nh·∫≠t technical details t·ª´ k·∫øt qu·∫£ ƒë√°nh gi√°
        if "technical_analysis" in evaluation_result:
            self.session_data["technical_details"].update(evaluation_result["technical_analysis"])
        
        return evaluation_result
    
    async def _build_evaluation_prompt(self, user_answer: str, last_question: str) -> str:
        """
        X√¢y d·ª±ng prompt ƒë·ªÉ ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi v√† l·∫≠p k·∫ø ho·∫°ch h√†nh ƒë·ªông ti·∫øp theo
        
        Args:
            user_answer: C√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng
            last_question: C√¢u h·ªèi cu·ªëi c√πng ƒë∆∞·ª£c h·ªèi
            
        Returns:
            Prompt cho LLM
        """
        # Format chat history
        chat_history_text = ""
        for i, exchange in enumerate(self.session_data["chat_history"], 1):
            chat_history_text += f"Q{i}: {exchange.get('question', '')}\n"
            chat_history_text += f"A{i}: {exchange.get('answer', '')}\n\n"
        
        # L·∫•y enrichment data g·∫ßn ƒë√¢y nh·∫•t n·∫øu c√≥
        enrichment_text = ""
        if USE_MCP_ENRICHMENT and self.session_data["context_enrichments"]:
            latest_enrichment = self.session_data["context_enrichments"][-1]["enrichment_data"]
            enrichment_text = format_enriched_context(latest_enrichment)
        
        # X√¢y d·ª±ng prompt
        prompt = f"""
B·∫°n l√† K·ªπ s∆∞ Tr∆∞·ªüng v·ªõi ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ thi·∫øt k·∫ø k·ªπ thu·∫≠t, ki·∫øn tr√∫c h·ªá th·ªëng, v√† ph√°t tri·ªÉn ph·∫ßn m·ªÅm. H√£y ph√¢n t√≠ch chi ti·∫øt th√¥ng tin hi·ªán t·∫°i:

Y√äU C·∫¶U BAN ƒê·∫¶U:
{self.session_data["requirement"]}

C√ÇU H·ªéI CU·ªêI C√ôNG:
{last_question}

C√ÇU TR·∫¢ L·ªúI M·ªöI NH·∫§T:
{user_answer}

L·ªäCH S·ª¨ TRAO ƒê·ªîI:
{chat_history_text}

{enrichment_text}

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. PH√ÇN T√çCH c√¢u tr·∫£ l·ªùi c·ªßa ng∆∞·ªùi d√πng t·ª´ g√≥c ƒë·ªô k·ªπ thu·∫≠t
2. ƒê√ÅNH GI√Å th√¥ng tin hi·ªán c√≥ ƒë√£ ƒë·ªß ƒë·ªÉ ph√°t tri·ªÉn ·ª©ng d·ª•ng ch∆∞a 
3. QUY·∫æT ƒê·ªäNH h√†nh ƒë·ªông ti·∫øp theo:
   - N·∫øu th√¥ng tin ƒê√É ƒê·ª¶: T·ªïng h·ª£p t·∫•t c·∫£ th√†nh b·∫£n technical requirement ho√†n ch·ªânh
   - N·∫øu th√¥ng tin CH∆ØA ƒê·ª¶: T·∫°o c√¢u h·ªèi ti·∫øp theo ƒë·ªÉ l√†m r√µ c√°c chi ti·∫øt k·ªπ thu·∫≠t c√≤n thi·∫øu

TI√äU CH√ç ƒê·ª¶ TH√îNG TIN:
- Chi ti·∫øt kƒ© thu·∫≠t ƒë·ªß ƒë·ªÉ k·ªπ s∆∞ b·∫Øt ƒë·∫ßu tri·ªÉn khai
- C√≥ hi·ªÉu r√µ ki·∫øn tr√∫c t·ªïng th·ªÉ c·ªßa h·ªá th·ªëng
- Technical stacks v√† framework ƒë√£ r√µ r√†ng
- Critical APIs v√† workflows ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a
- Database schema ho·∫∑c data structures ƒë√£ r√µ r√†ng
- Business logic v√† nghi·ªáp v·ª• ch√≠nh ƒë√£ ƒë∆∞·ª£c l√†m r√µ

Tr·∫£ v·ªÅ CH√çNH X√ÅC d∆∞·ªõi d·∫°ng JSON sau:
{{
  "updated_summary": "T·ªïng h·ª£p hi·ªÉu bi·∫øt hi·ªán t·∫°i v·ªÅ y√™u c·∫ßu k·ªπ thu·∫≠t",
  "technical_analysis": {{
    "architecture": "Ki·∫øn tr√∫c ƒë·ªÅ xu·∫•t",
    "components": ["Component1", "Component2"],
    "data_flow": "M√¥ t·∫£ lu·ªìng d·ªØ li·ªáu",
    "class_diagram": "ASCII diagram c·ªßa class diagram ch√≠nh",
    "sequence_diagram": "ASCII diagram c·ªßa sequence diagram ch√≠nh",
    "api_endpoints": ["API1", "API2"],
    "database_schema": "M√¥ t·∫£ schema",
    "integrations": ["Integration1", "Integration2"]
  }},
  "next_action": "ask_more ho·∫∑c sufficient",
  "next_question": "C√¢u h·ªèi k·ªπ thu·∫≠t chi ti·∫øt v·ªÅ tri·ªÉn khai ho·∫∑c workflow",
  "clarifying_questions": [
    {
      "question": "Chi ti·∫øt k·ªπ thu·∫≠t v·ªÅ tri·ªÉn khai c·ªßa ch·ª©c nƒÉng?",
      "technical_focus": "tri·ªÉn khai",
      "importance": "Cao" 
    },
    {
      "question": "Chi ti·∫øt c·ª• th·ªÉ v·ªÅ thu·∫≠t to√°n/c√¥ng ngh·ªá/API c·∫ßn s·ª≠ d·ª•ng?",
      "technical_focus": "c√¥ng ngh·ªá",
      "importance": "Trung b√¨nh"
    }
  ],
  "final_summary": "T√≥m t·∫Øt cu·ªëi c√πng v·ªõi chi ti·∫øt k·ªπ thu·∫≠t",
  "is_completed": true,
  "role_discussions": [
    { "role": "Qu·∫£n l√Ω S·∫£n ph·∫©m", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ Qu·∫£n l√Ω S·∫£n ph·∫©m" },
    { "role": "Ki·∫øn tr√∫c s∆∞", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ Ki·∫øn tr√∫c s∆∞" },
    { "role": "K·ªπ s∆∞", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ K·ªπ s∆∞" }
  ]
}}
"""
        return prompt
    
    async def finalize_requirements(self) -> Dict[str, Any]:
        """
        T·ªïng h·ª£p v√† ho√†n thi·ªán c√°c y√™u c·∫ßu k·ªπ thu·∫≠t cu·ªëi c√πng
        
        Returns:
            Dict ch·ª©a y√™u c·∫ßu k·ªπ thu·∫≠t cu·ªëi c√πng ƒë√£ ho√†n thi·ªán
        """
        self._start_new_cycle(OrchestratorState.SUMMARIZING)
        self.current_cycle.add_reasoning("T·ªïng h·ª£p y√™u c·∫ßu k·ªπ thu·∫≠t cu·ªëi c√πng")
        
        if not self.session_data["ready_to_conclude"]:
            self.current_cycle.add_reasoning("C·ªë g·∫Øng t·ªïng h·ª£p m·∫∑c d√π ch∆∞a s·∫µn s√†ng k·∫øt th√∫c")
        
        # X√¢y d·ª±ng prompt t·ªïng h·ª£p
        prompt = self._build_finalization_prompt()
        
        # G·ªçi LLM ƒë·ªÉ t·ªïng h·ª£p
        final_result = await self.llm.aask(prompt)
        self.current_cycle.add_llm_action(LLMAction.SUMMARIZE, {"prompt": "Finalize requirements", "result": final_result[:100] + "..."})
        
        # X·ª≠ l√Ω k·∫øt qu·∫£
        clean_response = final_result.strip()
        if clean_response.startswith("\n") and clean_response.endswith("\n"):
            clean_response = clean_response[3:-3].strip()
            if clean_response.startswith("json"):
                clean_response = clean_response[4:].strip()
        
        final_requirements = json.loads(clean_response)
        self.session_data["final_requirements"] = final_requirements
        
        self.current_cycle.complete(
            OrchestratorState.FINALIZING,
            "Ho√†n th√†nh t·ªïng h·ª£p y√™u c·∫ßu k·ªπ thu·∫≠t"
        )
        
        return final_requirements
    
# ==========================
# MCP CONTEXT ENRICHER
# ==========================

# Flag to√†n c·ª•c ƒë·ªÉ ki·ªÉm tra xem MCP enrichment c√≥ kh·∫£ d·ª•ng kh√¥ng
USE_MCP_ENRICHMENT = True  # M·∫∑c ƒë·ªãnh l√† True v√¨ code ƒë√£ ƒë∆∞·ª£c nh√∫ng v√†o main.py

async def extract_urls_from_text(text: str) -> List[str]:
    """
    Extract URLs from user text
    
    Args:
        text: User input text
        
    Returns:
        List of found URLs
    """
    import re
    url_pattern = r'https?://[^\s]+'
    return re.findall(url_pattern, text)

async def enrich_context_with_mcp(
    user_answer: str, 
    chat_history: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Enrich context by calling MCP tools with the user's latest answer
    Only enrich if user_answer contains technical keywords (not just confirmation/empty)
    """
    result = {
        "semantic_search_results": {},
        "code_search_results": {},
        "web_results": {},
        "intelligent_insights": [],
        "reasoning": []
    }

    # C√°c c√¢u x√°c nh·∫≠n ho·∫∑c kh√¥ng ch·ª©a th√¥ng tin k·ªπ thu·∫≠t
    confirmation_phrases = [
        "ok", "xong", "done", "ƒë·ªìng √Ω", "confirm", "x√°c nh·∫≠n", "kh√¥ng b·ªï sung", "kh√¥ng", "no", "yes", "ƒë·ªß", "ƒë∆∞·ª£c r·ªìi"
    ]
    # N·∫øu user ch·ªâ tr·∫£ l·ªùi x√°c nh·∫≠n ho·∫∑c c√¢u ng·∫Øn kh√¥ng c√≥ t·ª´ kh√≥a k·ªπ thu·∫≠t th√¨ b·ªè qua enrich
    if not user_answer or user_answer.strip().lower() in confirmation_phrases or len(user_answer.strip()) < 10:
        return result

    # Step 1: Extract technical keywords from user's answer
    try:
        # N·∫øu c√≥ LLM-based extraction th√¨ d√πng, kh√¥ng th√¨ fallback
        keywords = extract_keywords_simple_fallback(user_answer)
        print(f"‚úì Keywords extracted: {keywords}")
    except Exception as e:
        print(f"‚úó Keyword extraction failed: {e}")
        raise  # No fallback - raise exception to debug
    
    # N·∫øu kh√¥ng c√≥ keyword k·ªπ thu·∫≠t th√¨ kh√¥ng enrich
    if not keywords:
        print("‚Ñπ No technical keywords found - skipping enrichment")
        return result

    # Step 2: Perform semantic search for each keyword (t·ªëi ƒëa 3)
    for keyword in keywords[:3]:
        try:
            # Importing from mcp_client instead of mcp_tools
            from mcp_client import semantic_search
            print(f"‚è≥ Running semantic search for: {keyword}")
            sem_results = await semantic_search(query=keyword, context_type="code", limit=3)
            result["semantic_search_results"][keyword] = sem_results
            print(f"‚úì Semantic search completed for: {keyword}")
        except Exception as e:
            print(f"‚úó Semantic search failed for {keyword}: {e}")
            # Continue with other tools, don't abort on failure
    
    # Step 3: Search codebase for specific code implementations (t·ªëi ƒëa 2)
    for keyword in keywords[:2]:
        try:
            # Importing from mcp_client instead of mcp_tools
            from mcp_client import search_codebase
            print(f"‚è≥ Running code search for: {keyword}")
            code_results = await search_codebase(query=keyword, limit=2)
            result["code_search_results"][keyword] = code_results
            print(f"‚úì Code search completed for: {keyword}")
        except Exception as e:
            print(f"‚úó Code search failed for {keyword}: {e}")
            # Continue with other tools, don't abort on failure
    
    # Step 4: Extract and crawl URLs if present in user answer
    try:
        urls = await extract_urls_from_text(user_answer)
        if urls:
            print(f"‚úì URLs found in text: {urls}")
            # Importing from mcp_client instead of mcp_tools
            from mcp_client import crawl_and_summarize
            for url in urls:
                try:
                    print(f"‚è≥ Crawling webpage: {url}")
                    web_result = await crawl_and_summarize(url=url)
                    result["web_results"][url] = web_result
                    print(f"‚úì Web crawl completed for: {url}")
                except Exception as e:
                    print(f"‚úó Web crawl failed for {url}: {e}")
                    # Continue with other URLs, don't abort on failure
        else:
            print("‚Ñπ No URLs found in user answer")
    except Exception as e:
        print(f"‚úó URL extraction failed: {e}")
        # Continue with the result we have so far
    
    print(f"‚úì Enrichment complete. Results: {len(result['semantic_search_results'])} semantic searches, {len(result['code_search_results'])} code searches, {len(result['web_results'])} web crawls")
    return result

def extract_keywords_simple_fallback(text: str) -> List[str]:
    """
    Simple keyword extraction fallback when LLM-based extraction isn't available
    
    Args:
        text: Text to extract keywords from
        
    Returns:
        List of keywords
    """
    import re
    from collections import Counter
    
    # Remove punctuation and convert to lowercase
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    
    # Split into words
    words = text.split()
    
    # Filter out common stop words (basic list)
    stop_words = {'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 
                 'of', 'at', 'by', 'for', 'with', 'about', 'to', 't·ª´', 'v·ªõi', 'c√°c', 'nh·ªØng',
                 'l√†', 'v√†', 'n·∫øu', 'ho·∫∑c', 'b·ªüi', 'v√¨', 'cho', 'trong', 'n√†y', 'khi'}
    
    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
    
    # Count word frequencies
    word_counts = Counter(filtered_words)
    
    # Get top 5 most frequent words as keywords
    top_keywords = [word for word, _ in word_counts.most_common(5)]
    
    return top_keywords

def format_enriched_context(enriched_data: Dict[str, Any]) -> str:
    """
    Format enriched context into a string that can be added to LLM prompt
    
    Args:
        enriched_data: The data returned by enrich_context_with_mcp
        
    Returns:
        Formatted string with enriched context
    """
    formatted_text = "## ENRICHED CONTEXT FROM MCP TOOLS\n\n"
    
    # Add semantic search results
    if enriched_data.get("semantic_search_results"):
        formatted_text += "### SEMANTIC SEARCH RESULTS\n"
        
        for keyword, results in enriched_data["semantic_search_results"].items():
            formatted_text += f"\n#### SEARCH FOR: '{keyword}'\n"
            
            if "results" in results and results["results"]:
                for i, item in enumerate(results["results"][:3], 1):  # Limit to top 3 results per keyword
                    formatted_text += f"{i}. **{item.get('file', 'Unknown file')}**:\n"
                    formatted_text += f"   {item.get('content', 'No content')[:200]}...\n\n"
            else:
                formatted_text += "No relevant results found.\n\n"
    
    # Add code search results
    if enriched_data.get("code_search_results"):
        formatted_text += "### CODE IMPLEMENTATION REFERENCES\n"
        
        for keyword, results in enriched_data["code_search_results"].items():
            formatted_text += f"\n#### IMPLEMENTATION FOR: '{keyword}'\n"
            
            if "results" in results and results["results"]:
                for i, item in enumerate(results["results"][:2], 1):  # Limit to top 2 results per keyword
                    formatted_text += f"{i}. **{item.get('file', 'Unknown file')}**:\n"
                    formatted_text += f"\n{item.get('content', 'No content')[:300]}...\n\n"
            else:
                formatted_text += "No implementation references found.\n\n"
    
    # Add web results
    if enriched_data.get("web_results"):
        formatted_text += "### WEB REFERENCES\n\n"
        
        for url, result in enriched_data["web_results"].items():
            formatted_text += f"**Source: {url}**\n\n"
            
            if "summary" in result:
                formatted_text += f"{result['summary'][:500]}...\n\n"
            
            if "relevant_sections" in result:
                formatted_text += "**Key sections:**\n"
                for section in result["relevant_sections"][:3]:  # Limit to top 3 sections
                    formatted_text += f"- {section[:200]}...\n"
                formatted_text += "\n"
    
    return formatted_text

async def generate_enriched_prompt(
    original_prompt: str,
    user_answer: str,
    chat_history: List[Dict[str, Any]]
) -> str:
    """
    Generate an enriched prompt by adding MCP tool results to the original prompt
    
    Args:
        original_prompt: The original prompt to be enhanced
        user_answer: The latest user answer
        chat_history: Complete chat history
        
    Returns:
        An enhanced prompt with additional context
    """
    # Get enriched context from MCP tools
    print("‚è≥ Starting context enrichment with MCP tools...")
    enriched_data = await enrich_context_with_mcp(user_answer, chat_history)
    print("‚úì Context enrichment completed")
    
    # Format the enriched data for the prompt (non-async function)
    print("‚è≥ Formatting enriched context...")
    enriched_context = format_enriched_context(enriched_data)
    print(f"‚úì Formatting completed: {len(enriched_context)} characters")
    
    # Add the enriched context before the end of the prompt
    # Look for specific sections where it makes sense to insert our enriched context
    split_marker = "Tr·∫£ v·ªÅ CH√çNH X√ÅC d∆∞·ªõi d·∫°ng JSON" 
    
    if split_marker in original_prompt:
        parts = original_prompt.split(split_marker, 1)
        enhanced_prompt = (
            parts[0] + 
            "\n\n" + enriched_context + "\n\n" + 
            split_marker + 
            parts[1]
        )
    else:
        # Default to adding at the end if we can't find a good split point
        enhanced_prompt = original_prompt + "\n\n" + enriched_context
    
    return enhanced_prompt

class LLMOrchestrationResult:
    """
    L·ªõp wrapper ƒë·ªÉ ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ t·ª´ orchestrator th√†nh JSON
    ph√π h·ª£p ƒë·ªÉ tr·∫£ v·ªÅ qua WebSocket
    """
    
    def __init__(self, result_type: str, result_data: Dict[str, Any]):
        self.result_type = result_type
        self.result_data = result_data
        self.timestamp = datetime.utcnow().isoformat()
    
    def to_websocket_json(self) -> str:
        """Convert to JSON for websocket response"""
        
        if self.result_type == "question":
            # Return formatted question JSON
            return json.dumps({
                "text": f"ü§ñ K·ªπ s∆∞ Tr∆∞·ªüng: {self.result_data.get('main_question', '')}",
                "type": "question",
                "timestamp": self.timestamp,
                "clarifying_questions_text": self._format_clarifying_questions(),
                "main_question": self.result_data.get("main_question", ""),
                "clarifying_questions": self.result_data.get("clarifying_questions", [])
            })
        
        elif self.result_type == "conclusion":
            # Return formatted conclusion JSON
            return json.dumps({
                "text": f"üìã K·ªπ s∆∞ Tr∆∞·ªüng: D·ª±a tr√™n th√¥ng tin c·ªßa b·∫°n, t√¥i ƒë√£ t·ªïng h·ª£p ƒë∆∞·ª£c c√°c y√™u c·∫ßu k·ªπ thu·∫≠t nh∆∞ sau:",
                "type": "conclusion",
                "timestamp": self.timestamp,
                "technical_summary": self.result_data.get("updated_summary", ""),
                "technical_analysis": self.result_data.get("technical_analysis", {}),
                "is_completed": True
            })
        
        else:
            # Fallback to simple JSON
            return json.dumps({
                "text": f"ü§ñ K·ªπ s∆∞ Tr∆∞·ªüng: {self.result_data.get('main_message', 'C√≥ k·∫øt qu·∫£ m·ªõi')}",
                "type": self.result_type,
                "timestamp": self.timestamp,
                "data": self.result_data
            })
    
    def _format_clarifying_questions(self) -> str:
        """Format clarifying questions for display"""
        questions = self.result_data.get("clarifying_questions", [])
        if not questions:
            return ""
        
        result = "üîç **ƒê·ªÉ l√†m r√µ chi ti·∫øt k·ªπ thu·∫≠t, vui l√≤ng tr·∫£ l·ªùi:**"
        
        for i, q in enumerate(questions, 1):
            if isinstance(q, dict) and "question" in q:
                q_text = q["question"]
                # Th√™m focus k·ªπ thu·∫≠t n·∫øu c√≥
                if "technical_focus" in q and "importance" in q:
                    q_text = f"{q_text} [Focus: {q['technical_focus']}, M·ª©c ƒë·ªô: {q['importance']}]"
            else:
                q_text = str(q)
            
            result += f"\n{i}. {q_text}"
        
        return result

# Import MCP context enricher
try:
    from mcp_context_enricher import generate_enriched_prompt, enrich_context_with_mcp
    USE_MCP_ENRICHMENT = True
except ImportError:
    print("Warning: MCP context enrichment not available")
    USE_MCP_ENRICHMENT = False

# Import LLM Orchestrator - reflective agent core
try:
    from llm_orchestrator import LLMOrchestrator, LLMOrchestrationResult
    USE_LLM_ORCHESTRATOR = True
except ImportError:
    print("Warning: LLM Orchestrator not available")
    USE_LLM_ORCHESTRATOR = False

# Helper function to strip code blocks from LLM responses
def strip_code_block(text):
    """
    Removes markdown code block formatting from text.
    
    Args:
        text: The text potentially containing code blocks
        
    Returns:
        String with code block markers removed
    """
    # Check if text is None
    if not text:
        return ""
        
    # Remove markdown code block formatting
    pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
    matches = re.findall(pattern, text)
    
    if matches:
        return matches[0].strip()
    
    return text.strip()

# Import t·ª´ MetaGPT
# Roles
from metagpt.roles import Role, ProductManager, Architect, ProjectManager, Engineer

# Actions
from metagpt.actions import Action, UserRequirement, WriteDesign, WriteTasks, WriteCode

# Core components
from metagpt.schema import Message
from metagpt.logs import logger
from metagpt.llm import LLM
from metagpt.memory import Memory
from metagpt.team import Team
from metagpt.context import Context

# ƒê·ªãnh nghƒ©a JSON schemas ƒë·ªÉ t√°i s·ª≠ d·ª•ng
TECH_SPEC_JSON_SCHEMA = '''
{
  "system_overview": {
    "objective": "M·ª•c ti√™u v√† scope",
    "components": ["Th√†nh ph·∫ßn 1", "Th√†nh ph·∫ßn 2"],
    "deployment_model": "M√¥ h√¨nh tri·ªÉn khai"
  },
  "technical_architecture": {
    "sequence_diagrams": "ASCII diagram c·ªßa sequence",
    "class_diagrams": "ASCII diagram c·ªßa c√°c class ch√≠nh",
    "flow_diagrams": "ASCII diagram c·ªßa lu·ªìng x·ª≠ l√Ω",
    "design_patterns": ["Pattern 1", "Pattern 2"],
    "component_breakdown": ["Component 1 v√† m√¥ t·∫£ chi ti·∫øt", "Component 2 v√† m√¥ t·∫£ chi ti·∫øt"]
  },
  "tech_stack": {
    "frameworks": ["Framework 1", "Framework 2"],
    "libraries": ["Library 1", "Library 2"],
    "tools": ["Tool 1", "Tool 2"],
    "dependencies": ["Dependency 1", "Dependency 2"]
  },
  "api_specification": [
    {
      "endpoint": "/api/resource",
      "method": "POST",
      "request_body": { "field1": "type1", "field2": "type2" },
      "response_body": { "field1": "type1", "field2": "type2" },
      "authentication": "Ph∆∞∆°ng th·ª©c x√°c th·ª±c",
      "error_handling": "Chi ti·∫øt x·ª≠ l√Ω l·ªói"
    }
  ],
  "data_model": {
    "entity_relationship": "ASCII diagram c·ªßa ERD",
    "schema_definitions": [
      {
        "entity": "Entity1",
        "fields": [
          { "name": "field1", "type": "type1", "constraints": "constraints1" },
          { "name": "field2", "type": "type2", "constraints": "constraints2" }
        ],
        "relationships": ["Relationship 1", "Relationship 2"]
      }
    ],
    "indices": ["Index 1", "Index 2"],
    "migration_strategy": "Chi ti·∫øt migration strategy"
  },
  "implementation_details": {
    "functions": [
      {
        "name": "function_name",
        "signature": "def function_name(param1: type1, param2: type2) -> return_type",
        "module_path": "path.to.module",
        "pseudocode": "Chi ti·∫øt pseudocode",
        "algorithm": ["Step 1", "Step 2", "Step 3"],
        "edge_cases": ["Edge case 1", "Edge case 2"]
      }
    ],
    "directory_structure": ["folder1/", "folder1/file1.py", "folder2/"],
    "coding_standards": ["Standard 1", "Standard 2"]
  },
  "testing_strategy": {
    "unit_tests": "Chi ti·∫øt approach",
    "integration_tests": "Chi ti·∫øt approach",
    "mocks_fixtures": ["Mock 1", "Fixture 1"],
    "ci_cd": "Chi ti·∫øt pipeline"
  },
  "deployment": {
    "infrastructure": "Chi ti·∫øt setup",
    "containerization": "Chi ti·∫øt Docker/K8s",
    "monitoring_logging": "Chi ti·∫øt tools",
    "scaling": "Chi ti·∫øt strategy"
  }
}
'''

TASK_JSON_SCHEMA = '''
{
  "tasks": [
    {
      "id": 1,
      "title": "T√™n function/method c·ª• th·ªÉ v·ªõi namespace ƒë·∫ßy ƒë·ªß",
      "description": "M√¥ t·∫£ chi ti·∫øt v·ªÅ nhi·ªám v·ª• c·ªßa function n√†y, c√°c edge cases ph·∫£i x·ª≠ l√Ω, v√† logic nghi·ªáp v·ª• chi ti·∫øt",
      "priority": "high/medium/low",
      "estimated_hours": 2.5,
      "complexity": "high/medium/low",
      "type": "implementation/design/testing",
      "dependencies": ["ID c·ªßa c√°c task ph·∫£i ho√†n th√†nh tr∆∞·ªõc"],
      "required_skills": ["K·ªπ nƒÉng k·ªπ thu·∫≠t 1", "K·ªπ nƒÉng k·ªπ thu·∫≠t 2"],
      "function_details": {
        "file_path": "ƒê∆∞·ªùng d·∫´n file ch·ª©a function",
        "module_path": "module.submodule.file",
        "class_name": "T√™n class ch·ª©a method n√†y (n·∫øu √°p d·ª•ng)",
        "function_name": "T√™n ch√≠nh x√°c c·ªßa function/method",
        "signature": "def function_name(param1: type, param2: type) -> return_type:",
        "full_qualified_name": "module.submodule.class.function",
        "parameters": [
          {"name": "param1", "type": "string/int/Dict[str,Any]/etc", "description": "M√¥ t·∫£ chi ti·∫øt parameter", "validation": "C√°ch validate parameter n√†y"},
          {"name": "param2", "type": "string/int/etc", "description": "M√¥ t·∫£ chi ti·∫øt parameter", "default": "Gi√° tr·ªã default n·∫øu c√≥"}
        ],
        "return_value": {"type": "return_type", "description": "M√¥ t·∫£ chi ti·∫øt gi√° tr·ªã tr·∫£ v·ªÅ", "possible_values": ["C√°c gi√° tr·ªã c√≥ th·ªÉ tr·∫£ v·ªÅ"]},
        "algorithm_steps": [
          "1. B∆∞·ªõc 1: Chi ti·∫øt t·ª´ng b∆∞·ªõc c·ªßa thu·∫≠t to√°n",
          "2. B∆∞·ªõc 2: Chi ti·∫øt t·ª´ng b∆∞·ªõc c·ªßa thu·∫≠t to√°n",
          "3. B∆∞·ªõc 3: Chi ti·∫øt t·ª´ng b∆∞·ªõc c·ªßa thu·∫≠t to√°n"
        ],
        "edge_cases": ["Edge case 1 v√† c√°ch x·ª≠ l√Ω chi ti·∫øt", "Edge case 2 v√† c√°ch x·ª≠ l√Ω chi ti·∫øt"],
        "exceptions": [
          {"type": "ExceptionType1", "when": "Khi n√†o x·∫£y ra exception n√†y", "handling": "C√°ch x·ª≠ l√Ω"},
          {"type": "ExceptionType2", "when": "Khi n√†o x·∫£y ra exception n√†y", "handling": "C√°ch x·ª≠ l√Ω"}
        ],
        "design_patterns": ["Pattern 1 ƒë∆∞·ª£c √°p d·ª•ng", "Pattern 2 ƒë∆∞·ª£c √°p d·ª•ng"],
        "testing_strategy": "Chi ti·∫øt c√°ch test function n√†y"
      },
      "subtasks": [
        {
          "id": "1.1",
          "title": "Validate ƒë·∫ßu v√†o c·ªßa function",
          "description": "M√¥ t·∫£ chi ti·∫øt c√°ch validate t·ª´ng parameter",
          "estimated_hours": 0
        },
        {
          "id": "1.2",
          "title": "Ti√™u ƒë·ªÅ subtask (v√≠ d·ª•: X·ª≠ l√Ω core logic c·ªßa function)",
          "description": "M√¥ t·∫£ chi ti·∫øt subtask",
          "estimated_hours": 0
        },
        {
          "id": "1.3",
          "title": "Ti√™u ƒë·ªÅ subtask (v√≠ d·ª•: X·ª≠ l√Ω l·ªói v√† edge cases)",
          "description": "M√¥ t·∫£ chi ti·∫øt subtask",
          "estimated_hours": 0
        }
      ]
    }
  ],
  "assignments": [
    {
      "task_id": 1,
      "member_name": "T√™n th√†nh vi√™n",
      "assigned_hours": 0,
      "assignment_rationale": "L√Ω do ph√¢n c√¥ng"
    }
  ],
  "development_flow": {
    "phases": [
      {
        "name": "T√™n giai ƒëo·∫°n",
        "description": "M√¥ t·∫£ giai ƒëo·∫°n",
        "tasks": [1, 2, 3],
        "duration": "Th·ªùi gian ∆∞·ªõc t√≠nh"
      }
    ]
  },
  "team_summary": {
    "team_size": 0,
    "skill_coverage": ["K·ªπ nƒÉng 1", "K·ªπ nƒÉng 2"],
    "workload_distribution": "Ph√¢n b·ªï c√¥ng vi·ªác"
  }
}
'''

ANALYSIS_RESULT_JSON_SCHEMA = '''
{
  "updated_summary": "T·ªïng h·ª£p hi·ªÉu bi·∫øt hi·ªán t·∫°i v·ªÅ y√™u c·∫ßu",
  "technical_analysis": {
    "architecture": "Ki·∫øn tr√∫c ƒë·ªÅ xu·∫•t",
    "components": ["Component1", "Component2"],
    "data_flow": "M√¥ t·∫£ lu·ªìng d·ªØ li·ªáu",
    "class_diagram": "ASCII diagram c·ªßa class diagram ch√≠nh",
    "sequence_diagram": "ASCII diagram c·ªßa sequence diagram ch√≠nh",
    "api_endpoints": ["API1", "API2"],
    "database_schema": "M√¥ t·∫£ schema",
    "integrations": ["Integration1", "Integration2"]
  },
  "next_action": "ask_more ho·∫∑c sufficient",
  "next_question": "C√¢u h·ªèi k·ªπ thu·∫≠t chi ti·∫øt v·ªÅ tri·ªÉn khai ho·∫∑c workflow",
  "clarifying_questions": [
    {
      "question": "Chi ti·∫øt k·ªπ thu·∫≠t v·ªÅ tri·ªÉn khai c·ªßa ch·ª©c nƒÉng?",
      "technical_focus": "tri·ªÉn khai",
      "importance": "Cao" 
    },
    {
      "question": "Chi ti·∫øt c·ª• th·ªÉ v·ªÅ thu·∫≠t to√°n/c√¥ng ngh·ªá/API c·∫ßn s·ª≠ d·ª•ng?",
      "technical_focus": "c√¥ng ngh·ªá",
      "importance": "Trung b√¨nh"
    }
  ],
  "final_summary": "T√≥m t·∫Øt cu·ªëi c√πng v·ªõi chi ti·∫øt k·ªπ thu·∫≠t",
  "is_completed": true,
  "role_discussions": [
    { "role": "Qu·∫£n l√Ω S·∫£n ph·∫©m", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ Qu·∫£n l√Ω S·∫£n ph·∫©m" },
    { "role": "Ki·∫øn tr√∫c s∆∞", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ Ki·∫øn tr√∫c s∆∞" },
    { "role": "K·ªπ s∆∞", "highlights": "ƒêi·ªÉm nh·∫•n t·ª´ K·ªπ s∆∞" }
  ]
}
'''

app = FastAPI(title="MetaGPT Task Scheduler")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory="static"), name="static")

# Global storage for sessions and results
clarification_sessions: Dict[str, Dict[str, Any]] = {}
results_store: Dict[str, Dict[str, Any]] = {}
# Storage for tasks and requirements for unit test generation
project_data_storage: Dict[str, Dict[str, Any]] = {}

@app.get("/")
async def serve_ui():
    return FileResponse("static/index.html")

class TeamMember(BaseModel):
    name: str
    role: Optional[str] = None
    experience: Optional[int] = 0
    strengths: List[str] = Field(default_factory=list)

class ClarificationInitRequest(BaseModel):
    session_id: str
    general_requirement: str
    team_data: List[TeamMember]

@app.post("/init")
async def init_clarification(req: ClarificationInitRequest):
    clarification_sessions[req.session_id] = {
        "requirement": req.general_requirement,
        "team_data": [m.dict() for m in req.team_data],
        "chat_history": []
    }
    return {"status": "initialized", "session_id": req.session_id}

@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    if session_id not in clarification_sessions:
        await websocket.close(code=1008)
        return
    
    try:
        session = clarification_sessions[session_id]
        
        # S·ª≠ d·ª•ng LLM Orchestrator n·∫øu c√≥ s·∫µn, n·∫øu kh√¥ng th√¨ s·ª≠ d·ª•ng c√°ch c≈©
        if USE_LLM_ORCHESTRATOR:
            from llm_orchestrator import LLMOrchestrator, LLMOrchestrationResult, LLMAction
            
            # Kh·ªüi t·∫°o orchestrator
            llm_orchestrator = LLMOrchestrator()
            session["orchestrator"] = llm_orchestrator
            
            # G·ª≠i tin nh·∫Øn ch√†o
            welcome_json = json.dumps({
                "text": "ü§ñ K·ªπ s∆∞ Tr∆∞·ªüng: Ch√†o b·∫°n! T√¥i s·∫Ω trao ƒë·ªïi v·ªõi b·∫°n ƒë·ªÉ hi·ªÉu r√µ y√™u c·∫ßu k·ªπ thu·∫≠t c·ªßa d·ª± √°n.",
                "type": "welcome", 
                "timestamp": datetime.utcnow().isoformat()
            })
            await websocket.send_text(welcome_json)
            await asyncio.sleep(1)
            
            # Kh·ªüi t·∫°o phi√™n v·ªõi y√™u c·∫ßu ban ƒë·∫ßu
            await llm_orchestrator.initialize(session["requirement"], session["team_data"])
            
            # Sinh c√¢u h·ªèi ƒë·∫ßu ti√™n
            first_question = await llm_orchestrator.generate_first_question()
            
            # L∆∞u c√¢u h·ªèi ƒë·ªÉ tham chi·∫øu sau n√†y
            session["last_question"] = first_question.get("main_question", "")
            
            # T·∫°o k·∫øt qu·∫£ orchestration v√† g·ª≠i ƒëi
            first_result = LLMOrchestrationResult("question", first_question)
            await websocket.send_text(first_result.to_websocket_json())
            
            # V√≤ng l·∫∑p x·ª≠ l√Ω chat
            while True:
                user_message = await websocket.receive_text()
                
                # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng mu·ªën k·∫øt th√∫c
                if user_message.lower().strip() in ["done", "xong", "ok", "confirm", "x√°c nh·∫≠n"]:
                    if session.get("ready_to_confirm") or llm_orchestrator.session_data.get("ready_to_conclude"):
                        # Ho√†n thi·ªán requirements
                        final_requirements = await llm_orchestrator.finalize_requirements()
                        session["final_requirements"] = final_requirements
                        
                        confirm_json = json.dumps({
                            "text": "‚úÖ C·∫£m ∆°n b·∫°n! Ph·∫ßn clarification ƒë√£ ho√†n th√†nh.",
                            "type": "confirmation", 
                            "timestamp": datetime.utcnow().isoformat()
                        })
                        await websocket.send_text(confirm_json)
                        break
                    else:
                        clarify_json = json.dumps({
                            "text": "ü§î T√¥i c·∫ßn th√™m m·ªôt ch√∫t th√¥ng tin n·ªØa ƒë·ªÉ hi·ªÉu r√µ y√™u c·∫ßu. H√£y ti·∫øp t·ª•c tr·∫£ l·ªùi nh√©!",
                            "type": "clarify", 
                            "timestamp": datetime.utcnow().isoformat()
                        })
                        await websocket.send_text(clarify_json)
                        continue
                
                # N·∫øu ng∆∞·ªùi d√πng ƒë√£ x√°c nh·∫≠n nh∆∞ng mu·ªën b·ªï sung th√™m
                if session.get("ready_to_confirm") or llm_orchestrator.session_data.get("ready_to_conclude"):
                    # X·ª≠ l√Ω y√™u c·∫ßu b·ªï sung
                    process_result = await llm_orchestrator.process_user_answer(
                        user_message, 
                        "B·∫°n c√≥ mu·ªën b·ªï sung th√™m requirements n√†o kh√¥ng?"
                    )
                    
                    # C·∫≠p nh·∫≠t tr·∫°ng th√°i v√† g·ª≠i k·∫øt qu·∫£
                    if process_result["action"] == "conclude":
                        # Ng∆∞·ªùi d√πng ƒë√£ ho√†n th√†nh vi·ªác b·ªï sung
                        session["final_requirements"] = process_result.get("updated_summary", "")
                        session["ready_to_confirm"] = True
                        
                        result = LLMOrchestrationResult("conclusion", process_result)
                        await websocket.send_text(result.to_websocket_json())
                    else:
                        # Ng∆∞·ªùi d√πng ti·∫øp t·ª•c b·ªï sung, g·ª≠i c√¢u h·ªèi ti·∫øp theo
                        session["last_question"] = process_result.get("main_question", "")
                        
                        result = LLMOrchestrationResult("question", process_result)
                        await websocket.send_text(result.to_websocket_json())
                    
                    continue
                
                # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi th√¥ng th∆∞·ªùng
                last_question = session.get("last_question", "")
                process_result = await llm_orchestrator.process_user_answer(user_message, last_question)
                
                # C·∫≠p nh·∫≠t tr·∫°ng th√°i session
                session["last_question"] = process_result.get("main_question", "")
                
                if process_result["action"] == "conclude":
                    # Ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p ƒë·ªß th√¥ng tin
                    session["ready_to_confirm"] = True
                    session["final_requirements"] = process_result.get("updated_summary", "")
                    
                    result = LLMOrchestrationResult("conclusion", process_result)
                    await websocket.send_text(result.to_websocket_json())
                    
                    # G·ª≠i th√™m th√¥ng b√°o v·ªÅ vi·ªác ho√†n th√†nh
                    finalize_json = json.dumps({
                        "text": "üìã T√¥i ƒë√£ ghi nh·∫≠n ƒë·∫ßy ƒë·ªß y√™u c·∫ßu k·ªπ thu·∫≠t. B·∫°n c√≥ mu·ªën b·ªï sung th√™m g√¨ kh√¥ng tr∆∞·ªõc khi ti·∫øp t·ª•c?",
                        "type": "ready_to_confirm", 
                        "timestamp": datetime.utcnow().isoformat()
                    })
                    await asyncio.sleep(1)
                    await websocket.send_text(finalize_json)
                else:
                    # Ti·∫øp t·ª•c h·ªèi
                    result = LLMOrchestrationResult("question", process_result)
                    await websocket.send_text(result.to_websocket_json())
                    
                    # L∆∞u tr·∫°ng th√°i
                    session["current_summary"] = process_result.get("updated_summary", "")
        else:
            # === Ph∆∞∆°ng ph√°p c≈© n·∫øu kh√¥ng c√≥ Orchestrator ===
            # Kh·ªüi t·∫°o role K·ªπ s∆∞ Tr∆∞·ªüng t·ª´ MetaGPT (s·ª≠ d·ª•ng role Engineer)
            lead_engineer_role = Engineer()
            
            # T·∫°o context ƒë·ªÉ chia s·∫ª d·ªØ li·ªáu
            team_context = Context()
            # G√°n d·ªØ li·ªáu v√†o thu·ªôc t√≠nh kwargs c·ªßa Context
            team_context.kwargs.set("requirement", session["requirement"])
            team_context.kwargs.set("team_data", session["team_data"])
            
            # Kh·ªüi t·∫°o memory cho role
            lead_engineer_memory = Memory()
            
            # Thi·∫øt l·∫≠p role v·ªõi context v√† memory
            lead_engineer_role.context = team_context
            lead_engineer_role.memory = lead_engineer_memory
            
            # L∆∞u role v√†o session ƒë·ªÉ s·ª≠ d·ª•ng sau
            session["roles"] = {
                "lead_engineer": lead_engineer_role
            }
            session["team_context"] = team_context
            
            # --- MCP: Ph√¢n t√≠ch c·∫•u tr√∫c d·ª± √°n tr∆∞·ªõc khi role ·∫£o b·∫Øt ƒë·∫ßu ---
            try:
                # Ki·ªÉm tra xem mcp_client ho·∫∑c mcp_optimizer ƒë√£ ƒë∆∞·ª£c import ch∆∞a
                try:
                    # Th·ª≠ import t·ª´ mcp_optimizer (c√°ch m·ªõi)
                    from mcp_optimizer import analyze_requirement_with_mcp_auto_chaining
                    
                    # S·ª≠ d·ª•ng h√†m analyze_requirement_with_mcp_auto_chaining ƒë·ªÉ l·∫•y th√¥ng tin d·ª± √°n
                    mcp_analysis = await analyze_requirement_with_mcp_auto_chaining(
                    requirement=session["requirement"],
                    chat_history=[]
                )
                    
                except (ImportError, ModuleNotFoundError):
                    # Fallback: Th·ª≠ import t·ª´ mcp_client (c√°ch c≈©)
                    try:
                        from mcp_client import analyze_requirement_with_mcp_reasoning
                        mcp_analysis = await analyze_requirement_with_mcp_reasoning(
                            requirement=session["requirement"],
                            chat_history=[]
                        )
                    except (ImportError, ModuleNotFoundError):
                        # Fallback cu·ªëi c√πng: S·ª≠ d·ª•ng h√†m fallback
                        mcp_analysis = await fallback_mcp_analysis(session["requirement"])
                
                session["mcp_analysis"] = mcp_analysis
                # ƒê∆∞a th√¥ng tin n√†y v√†o context ƒë·ªÉ role ·∫£o "hi·ªÉu" c·∫•u tr√∫c d·ª± √°n
                team_context.kwargs.set("mcp_analysis", mcp_analysis)
            except Exception as e:
                logger.error(f"MCP server error: {e}")
                session["mcp_analysis_error"] = str(e)
            
            # B∆∞·ªõc 1: Kh·ªüi t·∫°o conversation n·∫øu ch∆∞a c√≥
            if not session.get("conversation_started"):
                # G·ª≠i tin nh·∫Øn ch√†o d∆∞·ªõi d·∫°ng JSON v·ªõi lo·∫°i l√† welcome
                welcome_json = json.dumps({
                    "text": "ü§ñ K·ªπ s∆∞ Tr∆∞·ªüng: Ch√†o b·∫°n! T√¥i s·∫Ω trao ƒë·ªïi v·ªõi b·∫°n ƒë·ªÉ hi·ªÉu r√µ y√™u c·∫ßu k·ªπ thu·∫≠t c·ªßa d·ª± √°n.",
                    "type": "welcome", 
                    "timestamp": datetime.utcnow().isoformat()
                })
                await websocket.send_text(welcome_json)
                await asyncio.sleep(1)
                
                # T·∫°o action cho vi·ªác t·∫°o c√¢u h·ªèi ban ƒë·∫ßu
                class GenerateInitialQuestionsAction(Action):
                    """Action ƒë·ªÉ t·∫°o c√¢u h·ªèi ban ƒë·∫ßu v·ªÅ requirements"""
                    
                    async def run(self):
                        # G·ªçi LLM ƒë·ªÉ sinh c√¢u h·ªèi ƒë·∫ßu ti√™n v√† c√°c c√¢u h·ªèi l√†m r√µ v·ªõi focus v√†o chi ti·∫øt k·ªπ thu·∫≠t ph√°t tri·ªÉn
                        llm_prompt = f"""
B·∫°n l√† K·ªπ s∆∞ Tr∆∞·ªüng. H√£y ƒë·ªçc y√™u c·∫ßu sau v√† sinh ra 1 c√¢u h·ªèi ch√≠nh ƒë·ªÉ l√†m r√µ v·ªÅ c√°c chi ti·∫øt k·ªπ thu·∫≠t, k√®m 2-3 c√¢u h·ªèi ph·ª• n·∫øu c·∫ßn.

Y√äU C·∫¶U: {session['requirement']}
TH√îNG TIN TEAM: {session['team_data']}

H∆Ø·ªöNG D·∫™N CHI TI·∫æT:
1. T·∫≠p trung v√†o c√°c kh√≠a c·∫°nh TRI·ªÇN KHAI K·ª∏ THU·∫¨T v√† CHI TI·∫æT PH√ÅT TRI·ªÇN c·ª• th·ªÉ
2. H·ªèi v·ªÅ stack c√¥ng ngh·ªá, ki·∫øn tr√∫c, database, API, integration, workflow k·ªπ thu·∫≠t...
3. H·ªèi chi ti·∫øt ƒë·∫øn m·ª©c c√°c class/function quan tr·ªçng n·∫øu c√≥ th·ªÉ
4. TR√ÅNH h·ªèi v·ªÅ c√°c gi·ªõi h·∫°n (limit) ho·∫∑c th·ªùi gian x·ª≠ l√Ω t·ªëi ƒëa
5. H·ªèi v·ªÅ c√°c requirement k·ªπ thu·∫≠t chi ti·∫øt m√† developer c·∫ßn ƒë·ªÉ tri·ªÉn khai

**B·∫ÆT BU·ªòC**: Ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, kh√¥ng s·ª≠ d·ª•ng ti·∫øng Anh, kh√¥ng gi·∫£i th√≠ch th√™m, kh√¥ng markdown/code block, kh√¥ng th√™m b·∫•t k·ª≥ k√Ω t·ª± n√†o ngo√†i JSON.

Tr·∫£ v·ªÅ JSON:
{{
  "main_question": "...",
  "clarifying_questions": ["...", "..."]
}}
"""
                initial_question_action = GenerateInitialQuestionsAction()
                llm_result = await initial_question_action.run()
                
                # Handle potential None result from LLM
                if llm_result is None:
                    logger.warning(f"LLM returned None for initial questions in session {session_id}")
                    first_question_data = {
                        "main_question": "B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m v·ªÅ c√°c y√™u c·∫ßu k·ªπ thu·∫≠t chi ti·∫øt c·ªßa d·ª± √°n kh√¥ng?",
                        "clarifying_questions": [
                            "B·∫°n ƒëang s·ª≠ d·ª•ng stack c√¥ng ngh·ªá n√†o?", 
                            "C√≥ y√™u c·∫ßu ƒë·∫∑c bi·ªát v·ªÅ hi·ªáu nƒÉng ho·∫∑c ki·∫øn tr√∫c kh√¥ng?"
                        ]
                    }
                else:
                    try:
                        first_question_data = json.loads(strip_code_block(llm_result))
                    except json.JSONDecodeError:
                        logger.error(f"Failed to parse JSON from LLM result: {llm_result}")
                        first_question_data = {
                            "main_question": "B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m v·ªÅ c√°c y√™u c·∫ßu k·ªπ thu·∫≠t chi ti·∫øt c·ªßa d·ª± √°n kh√¥ng?",
                            "clarifying_questions": [
                                "B·∫°n ƒëang s·ª≠ d·ª•ng stack c√¥ng ngh·ªá n√†o?", 
                                "C√≥ y√™u c·∫ßu ƒë·∫∑c bi·ªát v·ªÅ hi·ªáu nƒÉng ho·∫∑c ki·∫øn tr√∫c kh√¥ng?"
                            ]
                        }
                
                # L∆∞u k·∫øt qu·∫£ v√†o memory c·ªßa PM role
                session["roles"]["lead_engineer"].memory.add(Message(role="assistant", content=json.dumps(first_question_data)))
                
                session["conversation_started"] = True
                session["current_summary"] = ""
                
                # Ki·ªÉm tra xem k·∫øt qu·∫£ l√† JSON hay string
                if isinstance(first_question_data, dict):
                    main_question = first_question_data.get("main_question", "")
                    clarifying_questions = first_question_data.get("clarifying_questions", [])
                    
                    # L∆∞u c√¢u h·ªèi ch√≠nh
                    session["last_question"] = main_question
                    
                    # L∆∞u c√°c c√¢u h·ªèi l√†m r√µ ƒë·ªÉ s·ª≠ d·ª•ng sau
                    session["clarifying_questions"] = clarifying_questions
                    
                    # Chu·∫©n b·ªã v√† g·ª≠i c√¢u h·ªèi ch√≠nh d∆∞·ªõi d·∫°ng JSON v·ªõi lo·∫°i l√† question
                    # G·ªôp main question v√† clarifying questions th√†nh m·ªôt message c√≥ ƒë·ªãnh d·∫°ng t·ªët h∆°n
                    combined_question_text = f"ü§ñ K·ªπ s∆∞ Tr∆∞·ªüng: {main_question}"
                    
                    clarifying_text = ""
                    if clarifying_questions:
                        clarifying_text = "üîç **ƒê·ªÉ l√†m r√µ chi ti·∫øt k·ªπ thu·∫≠t, vui l√≤ng tr·∫£ l·ªùi:**"
                        for i, q_obj in enumerate(clarifying_questions, 1):
                            # Ki·ªÉm tra xem q_obj c√≥ ph·∫£i l√† dictionary kh√¥ng
                            if isinstance(q_obj, dict) and "question" in q_obj:
                                q_text = q_obj["question"]
                                # Th√™m focus k·ªπ thu·∫≠t n·∫øu c√≥, s·ª≠ d·ª•ng c√°ch an to√†n v·ªõi format() thay v√¨ f-string
                                if "technical_focus" in q_obj and "importance" in q_obj:
                                    focus = q_obj.get("technical_focus", "")
                                    importance = q_obj.get("importance", "")
                                    q_text = "{} [Focus: {}, M·ª©c ƒë·ªô: {}]".format(q_text, focus, importance)
                            else:
                                # N·∫øu l√† string ƒë∆°n gi·∫£n th√¨ s·ª≠ d·ª•ng lu√¥n
                                q_text = str(q_obj)
                            clarifying_text += "\n{}. {}".format(i, q_text)
                    
                    # T·∫°o JSON c√≥ c·∫•u tr√∫c r√µ r√†ng h∆°n
                    main_question_json = json.dumps({
                        "text": combined_question_text,
                        "type": "question", 
                        "timestamp": datetime.utcnow().isoformat(),
                        "clarifying_questions_text": clarifying_text,
                        "main_question": main_question,
                        "clarifying_questions": clarifying_questions
                    })
                    await websocket.send_text(main_question_json)
                else:
                    # Tr·ª±c ti·∫øp parse first_question_data
                    session["last_question"] = first_question_data
            
            # V√≤ng l·∫∑p chat t·ª´ng b∆∞·ªõc
            while True:
                user_message = await websocket.receive_text()
                
                # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng mu·ªën k·∫øt th√∫c
                if user_message.lower().strip() in ["done", "xong", "ok", "confirm", "x√°c nh·∫≠n"]:
                    if session.get("ready_to_confirm"):
                        confirm_json = json.dumps({
                            "text": "‚úÖ C·∫£m ∆°n b·∫°n! Ph·∫ßn clarification ƒë√£ ho√†n th√†nh.",
                            "type": "confirmation", 
                            "timestamp": datetime.utcnow().isoformat()
                        })
                        await websocket.send_text(confirm_json)
                        break
                    else:
                        clarify_json = json.dumps({
                            "text": "ü§î T√¥i c·∫ßn th√™m m·ªôt ch√∫t th√¥ng tin n·ªØa ƒë·ªÉ hi·ªÉu r√µ y√™u c·∫ßu. H√£y ti·∫øp t·ª•c tr·∫£ l·ªùi nh√©!",
                            "type": "clarify", 
                            "timestamp": datetime.utcnow().isoformat()
                        })
                        await websocket.send_text(clarify_json)
                        continue
                
                # X·ª≠ l√Ω ƒë·∫∑c bi·ªát khi ƒë√£ "sufficient" nh∆∞ng ng∆∞·ªùi d√πng mu·ªën b·ªï sung th√™m
                if session.get("ready_to_confirm"):
                    # L∆∞u c√¢u tr·∫£ l·ªùi b·ªï sung v√†o chat history
                    session["chat_history"].append({
                        "question": "B·∫°n c√≥ mu·ªën b·ªï sung th√™m requirements n√†o kh√¥ng?", 
                        "answer": user_message,
                        "timestamp": datetime.utcnow().isoformat()
                    })
                    
                    # Ng∆∞·ªùi d√πng ƒëang b·ªï sung th√™m
                    # L∆∞u y√™u c·∫ßu b·ªï sung
                    current_summary = session.get("current_summary", "")
                    updated_summary = f"{current_summary}\n\n**B·ªï sung th√™m:**\n- {user_message}"
                    session["current_summary"] = updated_summary
                    session["final_requirements"] = updated_summary
                    
                    # Ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung ƒë·ªÉ l√†m r√µ
                    class AnalyzeSuppRequirementAction(Action):
                        """Ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung v√† t·∫°o c√°c c√¢u h·ªèi l√†m r√µ"""
                        
                        async def run(self, requirement, new_requirement):
                            prompt = f"""
B·∫°n l√† Lead Technical Architect v√† Senior Software Engineer. Nhi·ªám v·ª•: ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung t·ª´ g√≥c ƒë·ªô k·ªπ thu·∫≠t chi ti·∫øt v√† t·∫°o c√°c c√¢u h·ªèi ƒë·ªÉ l√†m r√µ c√°c chi ti·∫øt TRI·ªÇN KHAI.

REQUIREMENT G·ªêC:
{requirement}

Y√äU C·∫¶U B·ªî SUNG M·ªöI:
{new_requirement}

H√£y ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung t·ª´ g√≥c ƒë·ªô K·ª∏ THU·∫¨T TRI·ªÇN KHAI v√†:
1. X√°c ƒë·ªãnh c√°c ƒëi·ªÉm ch∆∞a r√µ v·ªÅ m·∫∑t tri·ªÉn khai k·ªπ thu·∫≠t (classes, functions, API, database schemas...)
2. T·∫°o 2-3 c√¢u h·ªèi CHI TI·∫æT v·ªÅ m·∫∑t K·ª∏ THU·∫¨T (kh√¥ng h·ªèi v·ªÅ limits/constraints)
3. ƒê·ªÅ xu·∫•t c√°ch t√≠ch h·ª£p k·ªπ thu·∫≠t c·ª• th·ªÉ (structure, classes, interfaces...)
4. V·∫Ω s∆° ƒë·ªì tu·∫ßn t·ª± ho·∫∑c bi·ªÉu ƒë·ªì lu·ªìng x·ª≠ l√Ω b·∫±ng ASCII n·∫øu c√≥ th·ªÉ

Tr·∫£ v·ªÅ CH√çNH X√ÅC d∆∞·ªõi d·∫°ng JSON kh√¥ng c√≥ markdown code block (KH√îNG TH√äM 
json hay
):
{{
    "summary": "T√≥m t·∫Øt hi·ªÉu bi·∫øt v·ªÅ y√™u c·∫ßu b·ªï sung t·ª´ g√≥c ƒë·ªô k·ªπ thu·∫≠t",
    "technical_details": {{
        "classes": ["Class1", "Class2"],
        "apis": ["API1", "API2"],
        "database": "C·∫•u tr√∫c database v√† schemas"
    }},
    "clarifying_questions": [
        "C√¢u h·ªèi k·ªπ thu·∫≠t 1",
        "C√¢u h·ªèi k·ªπ thu·∫≠t 2",
        "C√¢u h·ªèi k·ªπ thu·∫≠t 3"
    ],
    "integration_suggestion": "ƒê·ªÅ xu·∫•t c√°ch t√≠ch h·ª£p k·ªπ thu·∫≠t",
    "flow_diagram": "ASCII diagram n·∫øu c√≥ th·ªÉ"
}}
"""
                            return await LLM().aask(prompt)
                    
                    try:
                        # Ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung
                        analyzer = AnalyzeSuppRequirementAction()
                        analysis_result_str = await analyzer.run(current_summary, user_message)
                        
                        # X·ª≠ l√Ω k·∫øt qu·∫£
                        analysis_result = {}
                        try:
                            # Parse JSON
                            import re
                            # Th·ª≠ parse tr·ª±c ti·∫øp
                            analysis_result = json.loads(strip_code_block(analysis_result_str))
                        except Exception as e:
                            logger.error(f"L·ªói khi parse JSON: {e}")
                            # Parse t·ª´ LLM response
                            analysis_result = {}
                            
                            # C·ªë g·∫Øng t·∫°o structure cho analysis_result t·ª´ response
                            if "summary" in analysis_result_str.lower():
                                analysis_result["summary"] = f"ƒê√£ ghi nh·∫≠n y√™u c·∫ßu b·ªï sung: {user_message}"
                            if "question" in analysis_result_str.lower():
                                analysis_result["clarifying_questions"] = []
                            if "integrat" in analysis_result_str.lower():
                                analysis_result["integration_suggestion"] = "T√≠ch h·ª£p v√†o h·ªá th·ªëng"
                        
                        # T·∫°o tin nh·∫Øn t·ªïng h·ª£p
                        combined_message = f"""üìù **BA AI**: T√¥i ghi nh·∫≠n th√™m requirement: '{user_message}'

üîç **Ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung:**
{analysis_result.get('summary', 'ƒê√£ ghi nh·∫≠n y√™u c·∫ßu b·ªï sung')}

‚ùì **M·ªôt s·ªë c√¢u h·ªèi ƒë·ªÉ l√†m r√µ th√™m:**
"""
                        
                        # Th√™m chi ti·∫øt k·ªπ thu·∫≠t n·∫øu c√≥
                        if analysis_result.get('technical_details'):
                            tech_details = analysis_result.get('technical_details', {})
                            combined_message += "\n\nüîß **Chi ti·∫øt k·ªπ thu·∫≠t:**\n"
                            
                            if tech_details.get('classes'):
                                combined_message += f"- **Classes/Components:** {', '.join(tech_details.get('classes'))}\n"
                            
                            if tech_details.get('apis'):
                                combined_message += f"- **APIs:** {', '.join(tech_details.get('apis'))}\n"
                            
                            if tech_details.get('database'):
                                combined_message += f"- **Database:** {tech_details.get('database')}\n"
                        
                        # Th√™m c√°c c√¢u h·ªèi l√†m r√µ
                        combined_message += "\n\n‚ùì **C√¢u h·ªèi k·ªπ thu·∫≠t chi ti·∫øt:**\n"
                        for i, q in enumerate(analysis_result.get('clarifying_questions', []), 1):
                            combined_message += f"{i}. {q}\n"
                        
                        # Th√™m ƒë·ªÅ xu·∫•t t√≠ch h·ª£p
                        combined_message += f"""
üîÑ **ƒê·ªÅ xu·∫•t t√≠ch h·ª£p k·ªπ thu·∫≠t:** 
{analysis_result.get('integration_suggestion', 'S·∫Ω ƒë∆∞·ª£c t√≠ch h·ª£p v√†o h·ªá th·ªëng')}
"""
                        
                        # Th√™m flow diagram n·∫øu c√≥
                        if analysis_result.get('flow_diagram'):
                            combined_message += f"""
üìä **S∆° ƒë·ªì lu·ªìng x·ª≠ l√Ω:**
{analysis_result.get('flow_diagram')}

"""
                        
                        combined_message += f"""
üìã **Requirements k·ªπ thu·∫≠t ƒë√£ c·∫≠p nh·∫≠t chi ti·∫øt:**
{updated_summary}

üìä **Task breakdown ƒë·∫øn m·ª©c function:**
- ƒê√£ ph√¢n chia c√°c tasks chi ti·∫øt ƒë·∫øn m·ª©c function
- M√¥ t·∫£ input/output c·ªßa t·ª´ng function
- S·∫Øp x·∫øp theo th·ª© t·ª± tri·ªÉn khai h·ª£p l√Ω
- X√°c ƒë·ªãnh dependencies gi·ªØa c√°c tasks

‚úÖ Vui l√≤ng tr·∫£ l·ªùi c√°c c√¢u h·ªèi l√†m r√µ ho·∫∑c b·ªï sung th√™m y√™u c·∫ßu k·ªπ thu·∫≠t chi ti·∫øt. B·∫•m 'X√°c nh·∫≠n & Ti·∫øp t·ª•c' khi b·∫°n ƒë√£ h√†i l√≤ng!"""

                    except Exception as e:
                        logger.error(f"L·ªói khi ph√¢n t√≠ch y√™u c·∫ßu b·ªï sung: {e}")
                        # Ti·∫øp t·ª•c v·ªõi y√™u c·∫ßu ƒë√£ c·∫≠p nh·∫≠t
                        combined_message = f"""üìù **Technical Analyst**: T√¥i ghi nh·∫≠n th√™m y√™u c·∫ßu k·ªπ thu·∫≠t: '{user_message}'

üìã **Requirements k·ªπ thu·∫≠t ƒë√£ c·∫≠p nh·∫≠t:**
{updated_summary}

üìä **Task breakdown s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t chi ti·∫øt ƒë·∫øn m·ª©c function/method**

‚úÖ Vui l√≤ng cho bi·∫øt n·∫øu b·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh hay b·ªï sung y√™u c·∫ßu k·ªπ thu·∫≠t. Nh·∫•n 'X√°c nh·∫≠n & Ti·∫øp t·ª•c' khi ƒë√£ s·∫µn s√†ng!"""

                    # G·ª≠i tin nh·∫Øn t·ªïng h·ª£p
                    await websocket.send_text(combined_message)
                    
                    continue  # Quay l·∫°i ƒë·∫ßu v√≤ng l·∫∑p ƒë·ªÉ ch·ªù input ti·∫øp theo
                
                # L∆∞u c√¢u tr·∫£ l·ªùi b√¨nh th∆∞·ªùng
                if session.get("last_question"):
                    session["chat_history"].append({
                        "question": session["last_question"], 
                        "answer": user_message,
                        "timestamp": datetime.utcnow().isoformat()
                    })
                    
                    # L∆∞u c√¢u tr·∫£ l·ªùi v√†o memory c·ªßa K·ªπ s∆∞ Tr∆∞·ªüng
                    user_message_obj = Message(role="user", content=user_message)
                    session["roles"]["lead_engineer"].memory.add(user_message_obj)
                    
                    # C·∫≠p nh·∫≠t context chung
                    session["team_context"].kwargs.set(f"user_answer_{len(session['chat_history'])}", user_message)
                
                # G·ª≠i th√¥ng b√°o ƒëang x·ª≠ l√Ω cho ng∆∞·ªùi d√πng
                processing_msg = json.dumps({
                    "text": "‚è≥ ƒêang ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n...",
                    "type": "processing", 
                    "timestamp": datetime.utcnow().isoformat()
                })
                await websocket.send_text(processing_msg)
            
    except Exception as e:
        logger.exception(f"WebSocket error: {str(e)}")
        await websocket.close(code=1011)


# T·∫°o action cho vi·ªác ph√¢n t√≠ch y√™u c·∫ßu v·ªõi nhi·ªÅu roles th·∫£o lu·∫≠n
class AnalyzeRequirementsAction(Action):
                """Action ƒë·ªÉ ph√¢n t√≠ch y√™u c·∫ßu v·ªõi m·ªôt role duy nh·∫•t - K·ªπ s∆∞ Tr∆∞·ªüng"""
                
                async def run(self, requirement, chat_history, team_data):
                    """Action ƒë·ªÉ ph√¢n t√≠ch y√™u c·∫ßu v·ªõi m·ªôt role duy nh·∫•t - K·ªπ s∆∞ Tr∆∞·ªüng"""
                    
                    try:
                        # --- Th·ª≠ s·ª≠ d·ª•ng context enrichment t·ª´ mcp_context_enricher n·∫øu c√≥ ---
                        enriched_context = ""
                        if USE_MCP_ENRICHMENT and chat_history and len(chat_history) > 0:
                            try:
                                # L·∫•y c√¢u tr·∫£ l·ªùi m·ªõi nh·∫•t c·ªßa ng∆∞·ªùi d√πng
                                latest_answer = chat_history[-1].get("answer", "")
                                
                                # L√†m gi√†u context v·ªõi MCP tools
                                print(f"L√†m gi√†u context v·ªõi c√¢u tr·∫£ l·ªùi m·ªõi nh·∫•t: '{latest_answer[:50]}...'")
                                enriched_data = await enrich_context_with_mcp(latest_answer, chat_history)
                                
                                # Format enriched context
                                from mcp_context_enricher import format_enriched_context
                                enriched_context = format_enriched_context(enriched_data)  # Not using await anymore
                                print("ƒê√£ l√†m gi√†u context th√†nh c√¥ng")
                            except Exception as e:
                                print(f"L·ªói khi l√†m gi√†u context: {e}")
                        
                        # Kh·ªüi t·∫°o mcp_analysis v·ªõi fallback
                        mcp_analysis = await fallback_mcp_analysis(requirement, chat_history)
                        
                        # Basic project info
                        project_info = "Basic project info"
                        if mcp_analysis.get("project_overview"):
                            project_info = f"Project info: {mcp_analysis['project_overview']}"
                        
                        # Simple prompt for testing
                        original_prompt = f"""
Ph√¢n t√≠ch y√™u c·∫ßu:
{requirement}

Th√¥ng tin d·ª± √°n:
{project_info}

{enriched_context}
"""
                        
                        # Enhanced prompt if available
                        enhanced_prompt = original_prompt
                        if USE_MCP_ENRICHMENT and chat_history and len(chat_history) > 0:
                            try:
                                latest_answer = chat_history[-1].get("answer", "")
                                enhanced_prompt = await generate_enriched_prompt(
                                    original_prompt=original_prompt,
                                    user_answer=latest_answer, 
                                    chat_history=chat_history
                                )
                                print("Enhanced prompt created")
                            except Exception as e:
                                print(f"Error creating enhanced prompt: {e}")
                        
                        # Get analysis from LLM
                        final_analysis = await LLM().aask(enhanced_prompt)
                        return final_analysis
                        
                    except Exception as e:
                        logger.error(f"Error in AnalyzeRequirementsAction.run: {e}")
                        return json.dumps({
                            "error": str(e),
                            "next_action": "ask_more",
                            "is_completed": False
                        })
            # End of AnalyzeRequirementsAction class

async def process_with_metagpt(session_id: str, requirement: str, team_data: List[Dict[str, Any]]):
    """
    X·ª≠ l√Ω y√™u c·∫ßu v·ªõi MetaGPT team
    
    Args:
        session_id: ID c·ªßa phi√™n l√†m r√µ y√™u c·∫ßu
        requirement: Y√™u c·∫ßu ƒë√£ l√†m r√µ
        team_data: Th√¥ng tin v·ªÅ team n·∫øu c√≥
    """
    try:
        logger.info(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω MetaGPT cho session {session_id}")
        results_store[session_id] = {
            "status": "processing", 
            "start_time": datetime.utcnow().isoformat()
        }

        # Kh·ªüi t·∫°o context chung
        team_context = Context()
        team_context.kwargs.set("requirement", requirement)
        team_context.kwargs.set("team_data", team_data)
        
        # Kh·ªüi t·∫°o team v·ªõi c√°c roles
        team_members = []
        
        # T·∫°o Product Manager role
        pm = ProductManager()
        pm.set_context(team_context)
        team_members.append(pm)
        
        # T·∫°o Architect role
        architect = Architect()
        architect.set_context(team_context)
        team_members.append(architect)
        
        # T·∫°o Project Manager role
        project_manager = ProjectManager()
        project_manager.set_context(team_context)
        team_members.append(project_manager)
        
        # T·∫°o Engineer role
        engineer = Engineer()
        engineer.set_context(team_context)
        team_members.append(engineer)
        
        # Kh·ªüi t·∫°o team
        team = Team(roles=team_members)
        
        # K√≠ch ho·∫°t team v·ªõi tin nh·∫Øn y√™u c·∫ßu
        message = Message(content=requirement, role="user")
        result = await team.run(message)
        
        # L∆∞u k·∫øt qu·∫£
        result_json = {
            "status": "completed",
            "end_time": datetime.utcnow().isoformat(),
            "design": result.get("design", ""),
            "tasks": result.get("tasks", []),
            "requirements": requirement,
            "team_members": team_data,
            "timestamp": datetime.utcnow().isoformat(),
            "status": "ready_for_unittest_generation"
        }
        results_store[session_id] = result_json
        logger.info(f"Successfully completed MetaGPT processing for session {session_id}")
    except Exception as e:
        logger.exception(f"Failed MetaGPT processing for session {session_id}: {e}")
        results_store[session_id] = {
            "status": "failed",
            "error": f"Processing error: {str(e)}"
        }
